\documentclass[]{report}
\usepackage[round,sort&compress,authoryear]{natbib}
\usepackage[usenames,dvipsnames]{color}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{hypcap}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}


% Set up some colors
\definecolor{gray92}{gray}{.92}
\definecolor{gray75}{gray}{.75}
\definecolor{gray45}{gray}{.45}

% Set up some PDF options and reference coloration
\hypersetup{
    pdftitle={Selected Topics},
    pdfauthor={Daniel E. Bruce},     % author
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

% listings settings
\lstset{
  breaklines=true,
  framerule=0.5pt,
  linewidth=\textwidth,
  numbers=left,
  showstringspaces=false
}

\lstdefinestyle{console}
{
  numbers=none,
  basicstyle=\bf\ttfamily,
  backgroundcolor=\color{gray92},
  frame=lrtb,
}

\lstset{style=console}


% Use "normal" paragraph separation
\setlength{\parskip}{1.3ex plus 0.2ex minus 0.2ex}
\setlength{\parindent}{0pt}

% Nicer margin comments
\let\oldmarginpar\marginpar
\renewcommand\marginpar[1]{\-\oldmarginpar[\raggedleft\footnotesize #1]%
{\raggedright\footnotesize #1}}


%
% Document begins here
%
\begin{document}
\title{Selected Topics regarding Automated Synthesis of NPCs}
\author{Daniel E. Bruce}
\date{\today}
\maketitle

\tableofcontents

\chapter{Introduction}
\label{cha:introduction}

This document covers a selection of the most relevant previous work related to
my Thesis with the working title ``Automated Synthesis of NPC AI with ADATE'',
on the topic of creating AI for game Non-Playing Characters with inductive
programming, using the ADATE system.

There are two main areas that have to be covered, specifically the topics of
Artificial Intelligence and Inductive Programming.

The work existing on the topic of Artificial Intelligence in games can be
roughly divided between work describing various implementation techniques, and
work describing how to make game AI more entertaining within the constraints of
existing implementation techniques.

The line between these two subtopics is very fuzzy and gradual, so some judgment
has been employed when putting the work into one category or the other, in some
cases the choice is purely symbolical, such as when the work describes a new
implementation technique created for the express purpose of increasing
entertainment value \citep[for example][]{khoo2002efficient}.

There is also some work on the more specific subtopic of using Evolutionary
Computing techniques such as Inductive Programming to improve game AI, either
for skill or entertainment value, which relates directly to the topic of my
thesis.

\chapter{AI techniques}
\label{cha:ai-techniques}

There is constant work on new AI techniques, both to make AI more efficient, and
to make AI seem smarter or more human. Familiarizing oneself with some of these
will allow better ability to choose an implementation technique that fits with
the goal of allowing automatic programming.

There is some differences between the techniques used in ``traditional'' AI and
game AI, primarily based on the performance of the various methods and their
focus on ability to solve problems and be entertaining, respectively. The two
areas seem to be converging, however, as traditional AI is starting to shift
from the goal of being ``really smart'' to making computers seem more human,
have an ability to learn and adapt, and maybe act like assistants, as detailed
in \citet[][]{ramos2008guest}.

When it comes to games (both video games and traditional games), the biggest
difference in how to do the AI comes from whether or not the players have
perfect information and whether there is chance involved. A perfect-information
game will require a completely different approach to AI than a
hidden-information or a stochastic game, and this will heavily influence what
type of AI is utilized, as mentioned in \citet[pg.~4-5]{schaeffer2002games}.

\section{Traditional AI techniques}
\label{sec:trad-ai-techn}

As noted in \citet[chap.~1]{munakata2008fundamentals} there are six main areas
of AI techniques, namely:

\begin{enumerate}
\item Symbolic AI (also ``traditional AI'')
\item Neural networks
\item Evolutionary Computing (under which Genetic Algorithms fall)
\item Fuzzy systems
\item Rough sets
\item Chaos
\end{enumerate}

Symbolic AI is the umbrella term for the traditional methods dating from the
field's inception, focusing on abstracting the world and applying logic and
rules to reach decisions.

The following two methods (Neural Networks and Evolutionary Computing) are
different reactions to the insufficiencies of Symbolic AI by attempting to more
closely modelling biological processes, so they can be called ``biology-inspired
AI''.

The remaining three methods are more recent developments which attempt to attack
the problem from a different angle, collectively called ``soft AI'' due to their
focus on not giving ``hard answers'' and use of uncertainty. These methods are
less relevant to the topic of my thesis, and as such will not be covered
further.

\subsection{History}
\label{sec:trad-ai-history}

Before going further it would be useful to go through the history of Artificial
Intelligence, to see its roots, and how closely it has developed alongside the
field of computer science itself. \citep{luger2005artificial,buchanan2002brief}

The field has traces back to antiquitiy with greek myths about artificial
beings, such as the golems and homunculus, and to the writings of many ancient
scholars (among others, Aristotle and Euclid) on the topic of reasoning, logic
and the mind. Further there have been many people working on mechanical
automaton, which tried to emulate human behaviour.

There was early work on intelligent machines from the very point where there
existed computers, but computer AI as a field didn't really come into existance
until 1950, through a pair of papers.

\begin{enumerate}
\item A very important paper was published by by Alan Turing named ``Computing
  machinery and intelligence'' \citep{turing1950computing}, where the he defined
  the ``Turing Test'' as a way to test whether a machine was truly intelligent.
\item Claude Shannon publishes a paper on programming a computer to play chess,
  by representing it as a search problem. \citep{shannon1950programming}
\end{enumerate}

The term ``artificial intelligence'' wasn't coined until six years later when
John McCarthy used the term for the first AI conference in 1956, where the first
running AI program (the Logic Theorist) was demonstrated.

It didn't take long for AI programs to challenge humans at board games, with the
first program to challenge a human world champion being made in 1962 by Arthur
Samuel, aimed at the game of checkers and utilizing machine learning to improve
its performance.

Further works into problem solving led to SAINT, that solves calculus at a
freshman level \citep{slagle1963heuristic}; ANALOGY, that solves the kind of
analogy questions found on IQ tests \citet{evans1964program}; and ELIZA, which
can simulate conversation \citep{weizenbaum1966eliza}.

Later, there was work into ``knowledge-based programs'' for artificial
reasoning, which created programs able to interpret mass spectra of chemical
compounds, solve integration problems in math, and play chess good enough to
reach a class-C rating.

In 1969, the beginnings of Neural Networks were appearing with
\citet{minsky1969perceptrons} which defined Perceptrons, while several papers on
natural language understanding were published during the next few years, along
with the introduction of ``expert systems'' using rule-based programming, and
the creation of the first computer to make a scientific discovery, the
Meta-Dendral learning program \citep{buchanan1976applications}.

Through the 70s and into the 80s, an explosion of well-known AI programs
occurred. Many ``expert systems'' were made that were capable of reasoning
within a limited space on the same level as a human expert, using the
traditional symbolic methods, before Neural Networks become widely used using
backpropagation (first introduced in \citet{werbos1974beyond}).

Around the 80s, the American Assosication for Artificial Intelligence (AAAI, now
named the Association for the Advancement of Artificial Intelligence after a
name change in 2007), which hosts many conferences and symposia each year, and
supports several journals on AI.

Another such boom happened in the 90s with major advances in all areas of AI,
with significant demonstrations in machine learning, intelligent tutoring,
case-based reasoning, multi-agent planning, scheduling, uncertain reasoning,
data mining, natural language understanding and translation, vision, virtual
reality, games, and other topics.

The 90s also had two significant events in games being played by AI, Deep Blue
beat Garry Kasparov in 1997, and TD-Gammon was written, that played backgammon
at championship-level. In addition, AI was starting to see use in cataloguing
the internet.

This brings us to today, with AI seeing use in toys (such as robotic pets), and
other forms of entertainment (most notably video games), but is still far away
from the goal of creating a human machine.

\subsection{Symbolic AI}
\label{sec:symbolic-ai}

This subfield of AI is also known as ``traditional'' or ``classic'' AI, and was
the approach that was used during the inception of AI, and is still heavily used
today. It is characterized by a top-down focus on logic and reasoning, and
relies on a symbolic description of the world, such as a set of rules, and is
thus said to be ``knowledge-based''.

Techniques included under Symbolic AI are knowledge-based systems, logical
reasoning, symbolic machine learning, search techniques, and natural language
processing.

\subsection{Biology-inspired AI}
\label{sec:biology-inspired-ai}

This term describes two subfields of AI, ``Neural Networks'' and ``Evolutionary
Computing'', both of which attempt to mimic biological processes, although not
in a way that attempts to emulate them faithfully. The two are also part of a
branch of Artificial Intelligence called Machine Learning, alongside techniques
such as ``Decision Trees'' and ``Bayesian Learning''.

\subsubsection{Neural Networks}
\label{sec:neural-networks}

Neural Networks, or more correctly Artificial Neural Networks (ANN) to separate
them from the Biological variety, are computer simulations that attempt to mimic
biological neural networks through a variety of algorithms, but they all revolve
around small units called ``neurons'' linked in networks of some sort, trained
by a learning algorithm.

The simplest variety of ANN is called a Feedforward Neural Network
\citep{wikipediafeedforward}, and consists of one or more layers of neurons
connected in a network where information only moves in one direction. The kind
of neuron most commonly used in such a network is the Perceptron
\citep{minsky1969perceptrons}, and one usually uses the Backpropagation
\citep{wikipediabackpropagation} algorithm to train these kinds of networks.

Other kinds of ANN are:
\begin{itemize}
\item Radial Basis Function Network.
\item Kohonen Self-organizing Network.
\item Learning Vector Quantization.
\item Recurrent Neural Network.
\item Modular Neural Networks.
\end{itemize}

ANNs are usually trained against a set of data containing both inputs and
outputs, using a learning algorithm that dictates how the neurons in the network
are to be updated based on the training data. The network is then tested against
a separate ``validation set'' where the input data is sent through the network
and compared with the output data to test the network's error rate. This process
is then repeated as long as necessary to generate an acceptable error rate.

ANN can used for multiple purposes, such as function approximation,
classification, and robotics, but require a large and diverse set of data to
train on to be useful, as ANN have a proclivity towards temporarily overfitting
if presented with a long series of less diverse data.

\subsubsection{Evolutionary Computing}
\label{sec:evol-comp}

In Evolutionary Computing the aim is to mimic the biological process of
evolution to evolve solutions to problems. There are many approaches that fall
under this name, among others Genetic Algorithms, Genetic Programming and
Evolutionary Algorithms, but the common trait is that they use the four
mechanics of biological evolution: reproduction, mutation, recombination and
selection.

These methods present the problem by taking a starting point towards a solution,
(which can be of any complexity, from nonexistant to a ``best-known'' program),
then ``reproduce'' it by applying a random set of mutations to generate a
certain amount of new solutions (called individuals). These individuals are then
tested with a fitness function defined by the person attempting to solve the
problem, which measures the individual's performance at the problem, and the
result of this is used to rank the individuals. We further choose one of the
individuals in the list, and ``reproduce'' to create new individuals either by
way of mutations or by using an algorithm to combine it with another individual
in a way that mimics the biological process of recombination. The newly
generated individuals are also tested for fitness and put into the list, then
the list is culled to a specific length by removing the least fit
individuals. This process is repeated until an optimal solution is found, or the
solution is deemed good enough.

The system used in my thesis, ADATE \citep{vattekar2006adate}, fits into a
subfield of Evolutionary Computing called Inductive Programming, which will be
described in Chapter \ref{cha:induct-progr}.

\section{Game AI techniques}
\label{sec:game-ai-techniques}

The approach to AI used in video games (as opposed to more traditional
board/card games) is very different from the academic approach, where AI
programs can take a long time to reach a decision, might require massive amounts
of resources and usually have as a goal to perform as excellently as possible.

In games the AI actors might only have a handful of microseconds of CPU time
available to reach a decision lest they impact the performance of the game, and
the AIs must fulfill the twin goals of being challenging (but beatable by the
majority of players) and entertaining (employ novel methods, and seem
human-like).

In addition, game AI is a comparatively recent field when compared to the volume
of research on AI work using traditional board and card games, which evolved
alongside computer science, as ``solving'' board games was one of the driving
forces behind computer science, as mentioned in \citet{schaeffer2002games}.

\subsection{History}
\label{sec:game-ai-history}

Before covering specific techniques, it is prudent to go through a history of AI
as used in games, to show the field's evolution in contrast to the field of
traditional AI which has a long history of strong scientific focus. The majority
of this information comes from \citet{tozour2002evolution}.

When it comes to video game AI, the methods employed have been, and still are,
marked by the heavy performance requirements and the fact that very little
emphasis has been put on AI sophistication until recently, as quipped about in
the following quote:

\begin{quote}
  Even today, game AI is haunted by the ghosts of Pac-Man's Inky, Pinky, Blinky
  and Clyde. Until very recently, the video game industry itself has done all
  too little to change this perception.
\end{quote}

Continuing on through the article, it is explained that many of the popular
early games used very crude AI, basically just a handful of simple rules, with
the exception of games that just digitized board games with well established AI
research, such as chess.

Sophisticated AI in video games was first embarked upon with turn-based strategy
games (such as \emph{Civilization}), then real times strategy games (such as
\emph{Age of Empires 2: The Age of Kings} and \emph{WarCraft II}). Further, good
AI started showing up in First Person Shooter games (\emph{Half-Life} and
\emph{Unreal: Tournament}) that showed tactical ability and the ability to model
several actors simultaneously, while \emph{Thief: The Dark Project} had actors
that emulated sense of sight and sound in a human-like fashion, and \emph{SWAT
  3: Close Quarters Battle} featured randomized AI parameters that allowed each
actor to have a slightly different personality every time the game is played.

After that the variety of different AI exploded, with games focusing entirely on
watching AI ``life'' grow, with \emph{SimCity} and \emph{The Sims} being very
well known staples, and \emph{Creatures} which is famous for being one of the
few games that actually uses a biological model for its ``Norns'', both
modelling biological processes with great detail and using neural networks for
the AI \citep[see][]{grand1997creatures}. Other games fit into the ``God games''
category, such as \emph{Populous} and \emph{Dungeon Keeper}, alongside
\emph{Black \& White}, which has the distinction of being the first major game
to focus the player's attention entirely on the game's AI capabilities, and
including a learning AI, a topic which is considered to be the next ``Big
Thing'' in gaming.

It should also be noted that for all the recent complexity, it is still the case
that the game AI community favours simple ``traditional'' methods implemented
through finite state machines, decision trees and rule systems, for their
excellent performance and relative simplicity. These are then further augmented
to add human-like behaviour, simulating planning and learning
\citep[see][]{orkin2003applying,isla2002new,khoo2002efficient,mateas2002behavior}.

\subsection{Traditional game AI}
\label{sec:traditional-game-ai}

Traditional game AI or ``simple AI'' is what game AI started out as, and still
to this day mostly uses. It's based upon simple rules or simple logical systems,
and has traditionally used a sampling of simple techniques, such as simply hard
coding the AI in a single routine, utilizing Finite State Machines, or using
rule-based systems. In addition, pathfinding has always been a topic with AIs,
where simple algorithms like A* have been dominant for a long time.

The common factor in traditional game AI techniques is that they tend to be
static, the NPCs are only capable of the things they were programmed to do
beforehand, and has little capability of learning or planning.


\subsubsection{Hard-coded AIs}
\label{sec:hard-coded-ais}

The first game NPCs utilized simple hard-coded AIs, which were basically a short
routine that ran every tick of the game with simple behaviours for the
opponents, not following any formalized methods for AI. This method is still
used to this day, as it's easy to create, and usually many minor opponents in
games don't need more sophistication than this method creates.

A good example of this technique is Pac-Man's ghosts, which used simple
pathfinding, and a selection of ``modes'' for the ghosts globally, then four
different targeting rules to give each ghost a ``personality'' as their entire
AI \citep{birch2010pacman,pittmanpac}. Even this very simple AI creates
interesting behaviours and engaging gameplay which propelled Pac-Man to one of
the most recognized games in video game history, evidence that you don't need
fancy high-powered neural nets to create interesting games.

\subsubsection{Finite State Machines}
\label{sec:finite-state-mach}

Finite State Machines (FSM) are the most used game AI technique, although the
FSM used by game developers do not necessarily work the same as the ones
described by Computer scientists. They take certain shortcuts which violate the
traditional definition of FSM, which make them more applicable to games
\citep{rabin2002implementing}.

FSM are used to formalize an NPC's behaviour in a simple way, as states and
transitions. States correspond to a specific behaviour, whereas a transition
correspond to a change in behaviour due to an event in the game. Using this, one
can easily map up simple behaviours that allow an NPC to act in a manner that
can be deemed ``intelligent,'' as long as the programmer thinks of all the
transitions necessary, and makes them seem natural.

This method of AI creation has the benefits of being simple to understand,
create and debug, as well as being very versatile by virtue of being a general
method that can lend itself to most any problem. Of course, there are downsides
to FSM as well. They can easily grow out of hand in more complex AI creations,
and they don't have the capability of combining states (so you can't have an NPC
be in the \emph{run away} and \emph{attack} states at the same time, to have it
do a tactical retreat, without explicitly programming the option in).

For a more thorough coverage of implementing an FSM, one can consult
\citet{rabin2002implementing} or \citet[][chap.~3]{kirby2011introduction}.

\subsubsection{Rule-based systems}
\label{sec:rule-based-systems}

As with FSM, Rule-based systems stem from traditional AI research, but is used
in a more loose form in video games \citep{christian2002simple}. The basic idea
of rule-based systems is that of a database (the data can be information,
actions or other things), where each piece of data has a ``matching rule'' which
is used by the system to infer which pieces of data apply to current situation.

In the context of games, this boils down to series of rules coupled with actions
or behaviours, many of which can apply at the same time, which together make up
the AI for the given NPC. These are formally named \emph{reaction rules}, and
are just one type of rule, with the other major one being \emph{consequent
  rules}, dealing mostly with information.

Usually there is some method of discerning which rules are the most relevant at
the current time, to prevent the NPC from doing too many things at once (many
simple AIs only allow the NPC to do a single action per tick), which can range
from randomly choosing a rule, to weighting each rule based on its specificity
and choosing the most applicable one \citep{Freeman-Hargis}.

The biggest benefit of this is that you can create a good set of behaviour with
a comparatively small amount of rules, so you aren't suspect to ``state
explosion'' as in an FSM, where adding a single new state will result in a
cascade of new transitions having to be written to handle every single case
where the NPC can switch in and out of the new state. In a rule-based system you
just add the new rule and write one matching function for it, and let your
system handle the rest.

Of course, the downside in this case is that creating a good AI like this
requires a bit more thinking to create a set of actions that acts well in the
most common cases, and still handles uncommon and unexpected cases decently
well, since one can't write a rule for every situation. This usually requires a
human with some expertise to either formulate or actually code the rules.

More info on writing rule-based systems can be found in
\citet[chap.~4]{kirby2011introduction}.

\subsection{Towards more human game AI}
\label{sec:towards-more-human}

In recent times there has been efforts in making game NPC act more human-like,
by introducing planning and learning behaviours (making the AI adaptable),
making their decisions be more unpredictable (but rational), culling any
obviously stupid behaviour, as well as engineering the NPC to introduce more
human elements such as adding the feeling of emotion and body-language
\citep{spronck2005adaptive}.

In the last decade or so, game AI has started taking into use planning and
learning behaviour to make NPC seem smarter and more human-like. Games such as
F.E.A.R (Using Goal-Oriented Action Planning as described in
\citet{orkin2006three}) use planning to make their opponents behave in a more
realistic manner, while other projects have used planning to make AI for
previously released games, such as WarCraft II (using Hierarchic Task Networks
as described in \citet{brickmanhtn}).

Planning allows the AI programmer to specify \emph{action sets} and \emph{goals}
which together make up the AI of the NPC. Action sets contain actions which
satisfy goals, and each action may have preconditions which in turn are
goals. Using this information, the NPC can plan its action by what it knows of
the world (which it can learn over time) and which actions it has defined,
instead of having to have the AI programmers code down every possible such path.

An example of the benefit of using a planning system comes from
\citet{orkin2006three}, where they describe an NPC which was working at a
computer being shot. In the traditional FSM-based system, each state had to be
exited fully before being able to transition, so when the NPC was shot, they
would exit their ``work'' state by turning off their computer, pushing out their
chair, and standing up, before promptly falling over dead. Not really what one
would consider human-like, or even believeable, behaviour. In a planning system,
that transition would be dynamically created, and the NPC would just slump over
dead from its chair, as one would expect.

Introducing emotion into games can be done in various ways, not just through
having the NPC itself show emotion, but also by having various ``AIs'' change
the mood of the game by altering lighting, music, and texture
\citep[chap.~9]{kirby2011introduction}.

NPC AI itself can also introduce emotions in the players by being very difficult
or very easy, or other behaviours giving rise to player experiences such as
``hard fun'', ``soft fun'' and ``serious fun'' \citep{lazzaro2004we}. NPC
difficulty can alter itself according to player skill to fulfill any of these
goals, something which many modern games utilize in what's called ``Difficulty
scaling'', which can refer to anything from simply giving you enemies with
higher levels (in RPG-style games such as Fallout 3), spawning more or less
enemies and changing where they appear (such as the AI Director in Left 4 Dead),
to evolving NPC AI that uses the player's performance as a parameter in a
fitness function to create better AI (one component of the thesis topic, and
will be described in more detail later)

\subsection{Advanced game AI}
\label{sec:advanced-game-ai}

More advanced game AI utilizes more heavy-duty AI techniques such as Genetic
programming and Neural Networks that learn and train as the game goes by, in an
effort to create an ever-changing experience for the gamer. The most well known
games which utilize these kinds of techniques are, as described earlier,
Creatures \citep{grand1997creatures} and Black \& White.

One technique used to allow these advanced agents is called Reinforcement
Learning \citep{merrick2006motivated,sutton1998reinforcement}, which differs
from the normal learning/training method of machine learning (termed Supervised
Learning) in that it isn't given any examples to learn from to learn how to
behave. In Reinforcement Learning the agents start out with a defined set of
actions and goals, and have to learn through trial and error which ones are the
best series of actions to fulfill a certain goal, which can easily change during
play as the human player changes its tactics.

Beyond this, the usage of more advanced AI techniques in games has been mostly
within the realm of research, after the games' release, either through altering
the source code of released games, or by creation of bots that use a certain AI
technique for research purposes and pitting it against humans and/or other bots
in a multiplayer setting.


\chapter{NPC Entertainment value}
\label{cha:npc-entert-value}

When AI is used for NPCs in games, the main goal is not to perfectly emulate a
human, nor to create the most skilled opponent possible, but rather to create an
opponent that SEEMS human, and possesses behaviour that creates an entertaining
experience, without being unduly challenging.

There are many methods of doing this, some of which have been described in
previous sections\marginpar{Link back?}. These range from simple
difficulty-scaling to match the player's skill level, through having NPCs that
perform ``human'' tasks such as planning and cooperating smartly, to having NPCs
and AI that show emotion and alter their behaviour and how they are perceived
based on their moods.

\section{Evaluating entertainment}
\label{sec:eval-entert}

Before you can make AI more entertaining, you need a way to measure if you're
actually reaching your goal. This can be done in one of two ways, you can
measure the response from real human beings playing your game, or you can try to
quantify entertainment value through various scientific means in an objective
manner.

\subsection{Measuring people's entertainment}
\label{sec:measuring-people}

Measuring a real human being's response to a game will give you the most
accurate measurement of entertainment, but it comes at a cost. The methods of
doing so can be very costly, in equipment, time, or both. There is also ample
room for bias either by the one doing the measurements or the measured person,
concious or subconscious.

Techniques employed here can range from very primitive to very advanced, and
include everything from surveys, to hooking a player up to gear to measure brain
waves.

Simple surveys or questionnares are easy and cheap to implement, but suffer from
not being able to capture complex data. In addition, and suffer from bias on the
part of the person filling out the data, where they subconsciously fill out data
closer to what they think you might be looking for than what they actually think
\citep{mandryk2006using}.

Another technique that can be used is recording a person playing the game, and
analyzing the recording to pull out data about how the person playing the game
feels about it. This can provide a very rich source of data, but suffers from
drawbacks, the biggest of which is that time spent analyzing is very high
compared to the amount of video recorded, ranging from 5 times to 100 times
longer. Another problem, as always, is bias, this time on part of the
researcher \citep{mandryk2006using}.

The previous techniques are mostly indirect measurements, but there are ways to
directly measure a person's enjoyment of a game, by utilizing physical measures
such as Galvanic Skin Response, Electrocardiography and respiration. This
involves hooking the players up to a series of equipment via scores of
electrodes and wires, and is thus a lot more expensive, time consuming and
intrusive. One way of doing this is described in \citet{mandryk2006using}.

\subsection{Quantifying entertainment}
\label{sec:quant-entert}

There is also work going into methods of quantifying entertainment in games,
with several articles proposing theories on what makes a game fun
\citep[e.g.][]{malone1981makes,read2002endurability,
federoff2002heuristics,lazzaro2004we,koster2004theory}, and others proposing methods for augmenting
or optimizing entertainment value in games
\citep[e.g.][]{yannakakis2009real,yannakakis2008model,yannakakis2007towards,yannakakis2004interactive}.

The presentation ``Why we play games'' \citep{lazzaro2004we} states that there
are four keys to making a fun game: ``Hard Fun'', in which the joy of overcoming
difficult challenges drives the player; ``Easy Fun'', where immersion into the
game world is more important than challenge; ``Altered States'', in which the
game elicits some change of the player's internal state; and ``The People
Factor'', where the social aspect of the game adds to, or creates, the
entertainment. These could be used as loose measures of how fun a game is, and
to whom it appeals, by quantifying the amount each of these keys are exhibited
in the game. These views are echoed and expanded heavily upon by
\citet{federoff2002heuristics}, which mostly comments on the first two keys, as
``challenge'' and ``immersion''.

A related view, as argued by
\citet{yannakakis2004interactive,yannakakis2004evolving}, is that interactive or
interesting opponents is the key to making fun games, and the key to having
interesting opponents is for them to adapt to the player, so that the player
doesn't get to a point where the AI is too simple for them. The key to doing
this, it's argued, is having a way of directly quantifying ``fun''.

As demonstrated in the reports, being able to do this quantification is
important for being able to evolve ``fun'' AI offline. It is prohibitively
difficul to make a fitness function for entertaining NPCs otherwise, as it is
unreasonable to expect to be able to have a player play against every new
individual and rate the AI's fitness.

The method utilized in the reports is to pit the evolved NPCs against a sampling
of pre-programmed AIs standing in for the player, and then using a
quantification formula of the NPC's interestingness and difficulty as the
fitness function or ``measure of fun''.

\section{Making entertaining NPC}
\label{sec:making-entert-npc}

As noted in the section \ref{sec:eval-entert}, one of the primary factors in a game NPC's
entertainment level is realized by having them be ``interactive'' opponents.
In that vein there is some work on increasing how human-like game AI is
behaving, as a way of increasing entertainment value, which involves planning
behaviours, anticipation, learning and adapting
\citep{orkin2004symbolic,orkin2003applying,
  yannakakis2009real,spronck2005adaptive}, as described in previous sections.

There is an approach called character-based AI collecting many of these
techniques \citep[][]{isla2002new}, which is gaining grounds in the game
industry as a way to give the game's AI agents a reasonable facsimile of human
behaviour and personality, or a ``character'', to make game opponents and allies
more entertaining. The term is applied to AI which collect many techniques and
behaviours that make the AI seem more human, like the aforementioned planning,
learning and adapting, but also the ability to sense patterns, the ability to
anticipate, and a certain model of the AI actor's perception that limits its
available information to a more ``realistic'' amount.

Another way of making NPCs more entertaining is by having them present a
constant challenge to the player, without them being too challenging. One way of
doing this is having the NPCs utilize Reinforcement Learning
\citep{merrick2006motivated,sutton1998reinforcement} to present a continuously
learning opponent. Another is to use one of the methods of difficulty scaling to
present different NPC opponents that correspond to the player's skill level, a
technique already utilized in many games at various levels of sophistication.

Another possible way of increasing an NPC's entertainment level is by increasing
the NPC's interestingness by making them act in interesting ways by way
automatic programming or other forms of genetic algorithms. This method would
utilize offline learning, and possibly online learning, to generate a selection
of NPC behaviours which would create a more interesting experience. This method
has been described by \citet{yannakakis2005ai}, and is going to be described in
more detail in Chapter \ref{cha:game-ai-via}.

\chapter{Inductive Programming}
\label{cha:induct-progr}

As noted in section \ref{sec:evol-comp}, ADATE \citep{olsson1995inductive} fits
into a sub-field of Artificial Intelligence called Inductive Programming, which is
actually a field recently emerging from being fragmented across Inductive Logic
Programming, Genetic Programming, and Machine Learning
\citep{kitzelmann2010inductive}.

\section{Introduction to Inductive Programming}
\label{sec:intr-induct-progr}

As described by the Approaches and Applications of Inductive Programming work
group \citep{aaip2010intro}, the aim of Inductive Programming is to create
programs based on incomplete specifications. These specifications are given in
the form of a set of inputs with matching outputs or an output evaluation
function that are used to evaluate a generated program's fitness. This is
opposed to Deductive approaches which start with a high-level description of the
wanted system, then have the system generate code that matches the
description.

The biggest difference between the two approaches is that the Deductive approach
guarantees that a solution is correct, although it requires the problem to be
well-understood and a specification to be written, whereas the Inductive
approach does not guarantee a fully correct solution, but only requires input
and output data to create its solution.

To help guide it towards an acceptable solution the Inductive Programming system
can also take other inputs in their specifications, in the form of constraints
on efficiency and complexity; background information about data types, functions
available for use in the generated code and information about the flow of the
intended program; heuristics or bias to guide the system towards a certain
solution. These are, however, not required.

As my thesis will be using the ADATE system, I will be explaining the details of
Inductive Programming in context of the ADATE system, as described in
\citet{olsson1994inductive} and \citet{vattekar2006adate}.

\section{The ADATE system}
\label{sec:adate-system}

ADATE's approach to Inductive Programming, formally named Evolutionary Inductive
Programming \citep{crossleycombining}, applies evolutionary computing to the
problem of making computer programs and the units which are manipulated (the
``genes'' if you will) are computer code operations. Apart from that, the
process which is followed is very much like the one described in the previous
section about evolutionary computing (section \ref{sec:evol-comp}).

All Inductive Programming system generate code in a specific programming
language, usually simplified or streamlined for the purpose, and ADATE is no
exception. It uses a subset of Standard ML, called ADATE-ML, for its generated
code, as well as for parts of its specification. \marginpar{Fill out details about ADATE ML here}

As mentioned before, all Inductive Programming systems make use of a
specification with the input and output necessary to frame the problem. ADATE's
specifications only make use of an output evaluation function, it does not
support, out of the box, the simpler variety of matching input data with output
data, although it is trivial to write an output evaluation function that works
in such a manner. ADATE also requires an initial program to be part of its
specification, although this program may be as simple as an empty function, as
the method it uses to generate programs is a form of local optimization, in
which a program is continuously improved to find better and better solutions.

ADATE's specification is written in two parts, one in ADATE ML and the other in
Standard ML, separated by a ``\%\%'' token.

The ADATE ML part contains three different things:
\begin{enumerate}
\item Definitions of data types and functions that can be used by the code that
  is generated by ADATE.
\item The initial program, called \(f\), that the ADATE system will start from
  and improve, which takes the inputs and returns an output.
\item A function \(main\) which is called by the ADATE's evaluation cycle, and
  can provide a scaffolding or a context around \(f\). This comes in handy
  if \(f\) is supposed to be a small part of a larger program, but the
  performance of the program is dependant on the performance of \(main\)
  itself, which may call \(f\) several times (perhaps not a constant or
  determinable amount) during its execution.
\end{enumerate}

The Standard ML part of the specification can contain any user supplied code
that might be used as part of the evaluation, but must define a certain set of
callbacks that are called by the ADATE system. These callbacks define things
such as the input data, any validation inputs, the set of functions that should
be available for generated code, the evaluation function, the parameters
governing amount and complexity of generated individuals, and other small
details (consult \citet[sect.~4.2]{vattekar2006adate} for details).

As ADATE is an evolutionary system, it uses the same general process as other
genetic systems, being that of a cycle of procreation, evaluation and culling in
the search for ever improving performance. These steps will now be described as
they apply to ADATE.

The process of creating new individuals in ADATE functions like most other
Evolutionary Computing solutions, in that it utilizes both mutation and genetic
crossover (albeit only in a limited fashion). When ADATE creates a new
individual it starts out with an individual from the ``kingdom'' existing
individuals (which at the beginning is only one program), then applies a series
of transformations called ``compound transformations'' to create the new
individual.

The six basic transformations available for use by ADATE are as follows:
\begin{description}
\item[R] is the only transformation that can actually change the semantics of a
  program, and it does this by replacing a part of the program with a new
  synthesized expression.
\item[REQ] is an \(R\) transformation that is guaranteed to not make the program
  worse.
\item[ABSTR] abstracts a piece of the program out into a separate function which
  can then later be reused by the program in several places.
\item[EMB] updates an existing function by adding a new parameter to it, and
  updates all calls to the function to take into account the new parameter.
\item[CASE-DIST] rearranges code within \(case\) and \(let\) expressions in a
  way such that semantics are preserved.
\item[Crossover] takes a series of REQ transformation and treats them as alleles
  in a genetic recombination.
\end{description}

The individuals are then evaluated by running the evaluation function specified
in the specification file on \(main\), using the current individual as \(f\),
over all the examples given. The evaluation value from this function is then
used to compute a few values regarding the individual's performance (among others
the time it takes to run and the complexity of the program).

If the individual evaluates better than any of the solutions currently in the
kingdom, the evaluation value is then used to group the individual with others
of similar performance in the kingdom. The kingdom is then culled by removing
the lowest ranked individual. The process then continues by creating a new
individual from the same base individual as before, until a given time limit is
elapsed, at which point a new base individual is chosen.

This loop continues forever until the program is halted, and output is written
to disk while the program runs. This output shows the current state of the
system, and the current best individuals along with their code. By evaluating
this output, the user of the system can decide if a good enough solution has
been created, or if the system needs to be stopped for other reasons.

\chapter{Evolving Game AI}
\label{cha:game-ai-via}

\chapter{Conclusion}
\label{cha:conclusion}

\bibliographystyle{abbrvnat}
\bibliography{topics}

\end{document}

% LocalWords:  ADATE
